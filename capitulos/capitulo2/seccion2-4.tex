\section{Diagonalización y triangularización de matrices.}

Ya con todas estas herramientas, podemos regresar a nuestro objetivo principal, el cual es saber cuando una matriz es diagonalizable o triangularizable. Primer demostremos algunos teoremas para un campo cualquiera, luego veremos algunos resultados para el campo de los complejos.

\subsection{Matrices diagonalizables}

Nuestro primer objetivo será dar una condición para encontrar cuando una matriz es diagonalizable, así como dar con un método para diagonalizarlas. Para ello consideremos la siguiente proposición.

\begin{prop}
  Sea $M \in \M_n(\F)$, entonces la matriz $M$ es diagonalizable si y solo sí existe alguna base $B$ de $\F^n$ tal que todos los elementos de $B$ son vectores propios de $M$.
\end{prop}
\begin{proof}
  Supongamos que $M$ es diagonalizable, es decir, existen matrices $P,D \in \M_n(\F)$, donde $D$ es diagonal y $P$ es invertible, tal que $M = P^{-1} D P$. Ahora, si $P^{-1} = \bigl( v_1 \mid \cdots \mid v_n\bigr)$ entonces por teoremas conocidos, $B = \{v_1,\ldots, v_n\}$ forma una base de $\F^n$. 

  De igual forma, por propiedades de inversa de una matriz, sabemos que $P v_i = e_i$ para todo $i \in \{1,\ldots,n\}$, de esta forma, notemos que para todo $i \in \{1,\ldots,n\}$ se cumple que
  \[
    Mv_i = (P^{-1}DP)v_i = P^{-1} D e_i = P^{-1} (D_{ii}e_i) = D_{ii} P^{-1}e_i = D_{ii} v_i.
  \]
  De esta forma $D_{ii}$ es un valor propio de $M$ con vector propio $v_i$ para todo $i \in \{1,\ldots,n\}$, así todos los elementos de la base $B$ son vectores propios de $M$.

  Ahora, supongamos que existe una base $B = \{v_1,\ldots,v_n\}$ donde todos sus elementos son vectores propios de $M$. Sea $\lambda_i$ el valor propio asociado al vector propio $v_i$ para $i \in \{1,\ldots,n\}$. Así, definamos las matrices 
  \[ D = \begin{spmatrix}{c|c|c} \lambda_1 e_i & \cdots & \lambda_n e_n \end{spmatrix} 
    \Eqand
     P = \begin{spmatrix}{c|c|c} v_1 & \cdots & v_n \end{spmatrix}.
  \]
  
  Por propiedades de las matrices, tenemos que $P$ es invertible ya que sus columnas forman una base de $\F^n$, de esta forma, dado que $P^{-1}v_i = e_i$ para todo $i\in\{1,\ldots,n\}$ por propiedades de la inversa de una matriz, entonces notemos que
  \begin{align*}
    P^{-1} M P &= P^{-1} M \begin{spmatrix}{c|c|c} v_1 & \cdots & v_n \end{spmatrix} 
       =P^{-1}  \begin{spmatrix}{c|c|c} Mv_1 & \cdots & Mv_n \end{spmatrix} \\
      &=P^{-1}  \begin{spmatrix}{c|c|c} \lambda_1 v_1 & \cdots & \lambda_n v_n \end{spmatrix} 
       =\begin{spmatrix}{c|c|c} P^{-1}(\lambda_1 v_1) & \cdots & P^{-1}(\lambda_n v_n) \end{spmatrix} \\
      &=\begin{spmatrix}{c|c|c} \lambda_1 P^{-1} v_1 & \cdots & \lambda_n P^{-1} v_n \end{spmatrix} 
       =\begin{spmatrix}{c|c|c} \lambda_1 e_1 & \cdots & \lambda_n e_n \end{spmatrix} \\
      &= D.
  \end{align*}
  De esta forma $D \sim M$ y por tanto $M$ es diagonalizable.
\end{proof}

Este teorema, nos da una condición para saber cuando una matriz es diagonalizable. Entonces, lo que tenemos que hacer es, primero, encontrar todos las raíces del polinomio característico y luego obtener vectores propios linealmente independientes. Para facilitar la búsqueda de esta base de vectores propios consideremos la siguiente definición.

\begin{defi}
  Sea $M \in \M_n(\F)$ y $\lambda \in E(M)$, al conjunto $E_\lambda = \{v\in\F : Mv = \lambda v\}$ le llamaremos el \emph{espacio propio} de $\lambda$.
\end{defi}

Notemos que el espacio propio de un valor propio están compuesto por todos los vectores propios asociados a este y el vector cero. Además, este conjunto, como su nombre lo indica, es un subespacio vectorial de $\F^n$. Esto se puede comprobar ya que para cualesquiera $v,w \in V_\lambda$ y $c \in \F$ se cumple que $v+cw \in V_\lambda$, ya que
  \[
    (v+cw)M = Mv + cMw = \lambda v + c \lambda w = \lambda (v+cw).
  \]
\begin{prop} \label{prop:BVPDiag}
  Sea $M \in \M_n(\F)$ y $\lambda \in E(M)$, entonces el espacio propio de $\lambda$ es un subespacio vectorial de $\F^n$. Más aún $E_\lambda = \ker(M-\lambda I)$.
\end{prop}
\begin{proof}
  Ya vimos que $E_\lambda$ es un espacio propio, de este modo solo falta probar la segunda afirmación. Ahora, notemos que 
  \[ Mv = \lambda v \iff (M-\lambda I)v = \bec 0. \]
  De aquí podemos comprobar de manera simple que $E_\lambda = \ker(M-\lambda I)$.
\end{proof}

Con esta propiedad, uno puede inferir que para encontrar vectores propios linealmente independientes se puede buscar bases de $\ker(M-\lambda I)$ para cada $\lambda \in E(M)$ y tomar las que sean linealmente independientes. Este proceso se puede simplificar con el siguiente teorema.

\begin{teor}\label{teor:IndepEP}
  Sea $M \in \M_n(\F)$ y $E(M) = \{\lambda_1, \ldots, \lambda_k\}$, entonces $E_{\lambda_1} \oplus \cdots \oplus E_{\lambda_k}$.
\end{teor}
\begin{proof}
  Por inducción, probemos que para todo $1\leq m \leq k$, si $v_1 + \cdots + v_m = \bec 0$, donde $v_i \in E_{\lambda_i}$ para cada $i\in\{1,\ldots,m\}$, entonces $v_1 = \cdots = v_m = \bec 0$. Es claro que si $m = 1$, entonces $v_1 = 0$. Así, supongamos que existe $1 \leq s < n$ tal que se sostiene la propiedad.

  Sea $v_i \in E_{\lambda_i}$ para cada $i\in\{1,\ldots,s+1\}$ tal que $v_1 + \cdots+ v_{s+1} = \bec 0$, por definición tenemos que
  \begin{align*}
    \bec 0 &= M\bec 0 = M(v_1 + \cdots+ v_{s+1}) \\
      &= Mv_1 + \cdots+ Mv_{s+1} \\
      &= \lambda_1v_1 + \cdots+ \lambda_{s+1}v_{s+1}.
  \end{align*}
  De esta forma, dado que $\lambda_{s+1} v_1 + \cdots+ \lambda_{s+1} v_{s+1} = \lambda_{s+1}\bec 0 = \bec 0$, entonces tenemos que 
    \begin{align*}
      \bec 0 &= (\lambda_1v_1 + \cdots+ \lambda_{s+1}v_{s+1}) - (\lambda_{s+1} v_1 + \cdots+ \lambda_{s+1} v_{s+1}) \\
        &= (\lambda_1 - \lambda_{s+1})v_1 + \cdots + (\lambda_s - \lambda_{s+1})v_s.
    \end{align*}
  
    Ahora, notemos que $(\lambda_i - \lambda_{s+1})v_{i} \in E_{\lambda_i}$ para cada $i \in \{1,\ldots,s\}$, de este modo, por hipótesis de inducción, tenemos que
      \[ (\lambda_1 - \lambda_{s+1})v_1 = \cdots = (\lambda_s - \lambda_{s+1})v_s = \bec 0.\]
    Ahora, como  $\lambda_i \neq \lambda_{s+1}$, para todo $i \in \{1,\ldots,s\}$, por construcción, entonces podemos concluir que $v_1 = \cdots = v_s = \bec 0$. Finalmente, por lo ya demostrado, tenemos que 
      \[ \bec 0 = v_1 + \cdots+ v_s + v_{s+1} = v_{s+1}, \]
    mostrando así que la propiedad se sostiene para $s+1$ y por tanto para todo $1 \leq m \leq k$.

    Ahora, tomando el caso particular $m = k$, notemos que si tomamos $v_i \in E_{\lambda_i}$ para cada $i \in \{1\ldots,k\}$ tal que $v_1 + \cdots + v_n$, entonces $v_1 = \cdots = v_k = \bec 0$, pero por definición, esto implica que $E_{\lambda_1}, \ldots, E_{\lambda_k}$ son independientes y por tanto $E_{\lambda_1} \oplus \cdots \oplus E_{\lambda_k}$.
\end{proof}

Con este teorema tenemos finalmente la ultima pieza para diagonalizar una matriz. En primer lugar podemos encontrar los valores propios de una matriz $M\in M_n(\F)$ con el polinomio característico, luego con estos podemos encontrar bases para los espacios $E_\lambda = \ker(M-\lambda I)$, dado que son independientes, al juntar todas las bases obtenemos un conjunto linealmente independientes. Si este conjunto linealmente independientes tiene exactamente $n$ términos, entonces tenemos que la matriz es diagonalizable. Todo este proceso se puede resumir con el siguiente teorema.

\begin{teor}\label{teor:DiagCond}
  Sea $M = \M_n(\F)$, entonces $M$ será diagonalizable si y solo si alguna de las siguiente condiciones se satisface.
  \begin{enumerate}
    \item $\dim(E_{\lambda_1}) + \cdots + \dim(E_{\lambda_k}) = n$ donde $E(M) = \{\lambda_1, \ldots, \lambda_k\}$.
    \item $\abs{E(M)} = n$.
  \end{enumerate}
\end{teor}
\begin{proof}
  En primer lugar, si $\dim(E_{\lambda_1}) + \cdots + \dim(E_{\lambda_k}) = n$ entonces $\F^n = E_{\lambda_1} \oplus \cdots \oplus E_{\lambda_k}$, pero eso implica que puedo encontrar una base de $\F^n$ conformada de solo vectores propios de $M$. Y ya sabemos por la proposición \ref{prop:BVPDiag} que esto implica que $M$ es diagonalizable.

  Ahora si $\abs{E(M)} = n$ dado que $\dim(E_{\lambda_1}) \geq 1$, entonces tenemos que
  \[ n = \dim(\F^n) \geq \dim(E_{\lambda_1}) + \cdots + \dim(E_{\lambda_n}) \geq n, \]
  así $\dim(E_{\lambda_1}) + \cdots + \dim(E_{\lambda_n}) = n$ y por la primera parte tenemos que $M$ es diagonalizable.

  Ahora, supongamos que $M$ es diagonalizable entonces por la proposición \ref{prop:BVPDiag} existe una base de $\F^n$ conformada por vectores propios de $M$. Ahora, notemos que $\dim(E_{\lambda_1}) + \cdots + \dim(E_{\lambda_k}) \leq n$ es la mayor cantidad de vectores propios linealmente independientes, por el teorema \ref{teor:IndepEP}. De aquí, para que haya una base conformada de vectores propios, entonces tenemos que $\dim(E_{\lambda_1}) + \cdots + \dim(E_{\lambda_k}) = n$.
\end{proof}

\begin{example}
  Consideremos la matriz $M$ con entradas reales de $3\times 3$ dada por
  \[
    M = \begin{pmatrix} -9 & 4 & 4 \\ -8  & 3 & 4 \\ -16 & 8 & 7 \end{pmatrix}.
  \]
  Mostremos que $M$ es diagonalizable y encontremos la matriz que la diagonaliza.

  \examplesolution

  Primero encontremos todos los valores propios de la matriz usando el polinomio característico. Veamos que
  \[
  \det(M-\lambda I) = \det\begin{pmatrix} -9 - \lambda & 4 & 4 \\ -8  & 3 - \lambda & 4 \\ -16 & 8 & 7 - \lambda \end{pmatrix} 
    = -\lambda^3+\lambda^2+5\lambda+3 = -(\lambda+1)^2(\lambda-3).
  \]
  De aquí obtenemos que $E(M) = \{-1, 3\}$. Ahora encontremos bases para $E_{-1} = \ker(M + I)$ y $E_{3} = \ker(M - 3I)$.

  Primero, para $\lambda = -1$, por eliminación gaussiana, notemos que
  \[
    M + I = \begin{pmatrix} -8 & 4 & 4 \\ -8  & 4 & 4 \\ -16 & 8 & 8 \end{pmatrix}
      \xrightarrow{\text{FERR}} \begin{pmatrix} 1 & -1/2 & -1/2 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}
  \]
  De aquí podemos ver que $\{(1,2,0)^t, (1,0,2)^t\}$ conforma una base de $E_{-1} = \ker(M + I)$.

  Análogamente para $\lambda = 3$, por eliminación gaussiana, notemos que
  \[
    M - 3I = \begin{pmatrix} -12 & 4 & 4 \\ -8  & 0 & 4 \\ -16 & 8 & 4 \end{pmatrix}
      \xrightarrow{\text{FERR}} 
      \begin{pmatrix} 1 & 0 & -1/2 \\ 0 & 1 & -1/2 \\ 0 & 0 & 0 \end{pmatrix}
  \]
  De aquí podemos ver que $\{(1,1,2)\}$ conforma una base de $E_{3} = \ker(M - 3I)$.

  Usando el teorema \ref{teor:DiagCond} ya es fácil de ver que $M$ es diagonalizable, ya que se cumple que $\dim(E_{-1}) + \dim(E_{3}) = 3$. Ahora, para ver a que matriz diagonal $M$  es semejante, tenemos que repetir los pasos de la demostración de la vuelta de la proposición \ref{prop:BVPDiag}.

  Así, consideremos la matriz $P$ donde sus columnas son las bases de $E_{-1}$ y $E_{3}$, por propiedades conocidas $P$ será invertible y además
    \[
      P = \begin{pmatrix}
        1 & 1 & 1 \\
        2 & 0 & 1 \\
        0 & 2 & 2
      \end{pmatrix}
        \Eqand
      P^{-1} = \begin{pmatrix} 1 & 0 & -1/2 \\ 2 & -1 & - 1/2 \\ -2 & 1 & 1 \end{pmatrix}.
    \]
  Realizando un simple cálculo podemos comprobar que $P$ diagonaliza a $M$, ya que
    \[
      P^{-1}MP = \begin{pmatrix} -1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 3 \end{pmatrix}.
    \]
\end{example}

\subsection{Triangularización de matrices complejas}

Ya vimos las condiciones necesarias para que una matriz se diagonalizable. Ahora, no toda matriz se puede diagonalizar, ni siquiera toda matriz se puede triangular, para mostrar un ejemplo de esto demostremos el siguiente lema.

\begin{lema}\label{lema:CleanColumn1}
  Sea $M \in \M_n(\F)$. La matriz $M$ tiene algún valor propio $\lambda$ si y solo sí es semejante a alguna matriz de la forma
  \[
    N = \begin{spmatrix}{c|c|c|c}
      \lambda e_1 & w_2 &  \ldots & w_n
    \end{spmatrix},
  \]
  donde $w_2, \ldots, w_n \in \F^{n-1}$.
\end{lema}
\begin{proof}
  Supongamos que $M$ tiene algún valor propio $\lambda$, de este modo existe vector no nulo $v$ tal que $Mv = \lambda v$. Ahora, por propiedades de los $\F$-espacios vectoriales, sabemos que debe existir vectores $v_2,\ldots,v_n \in \F^n$ tal que $B = \{v,v_2,\ldots,v_n\}$ es una base de $\F^n$.

  De esta forma, consideremos la matríz $P = \bigl( v \mid v_2 \mid \cdots \mid v_n \bigr)$, en primer lugar, notemos que $P$ es invertible ya que sus columnas forman una base y por propiedades de la inversa $P^{-1}v = e_1$. De este modo, sea $N = P^{-1}MP$, notemos que 
  \begin{align*}
    N &= P^{-1} M \begin{spmatrix}{c|c|c|c} v & v_2 &  \ldots & v_n \end{spmatrix} 
       = P^{-1} \begin{spmatrix}{c|c|c|c} Mv & Mv_2 &  \ldots & Mv_n \end{spmatrix} \\
      &= P^{-1} \begin{spmatrix}{c|c|c|c} \lambda v & Mv_2 &  \ldots & Mv_n \end{spmatrix} 
       =  \begin{spmatrix}{c|c|c|c} P^{-1}(\lambda v) & P^{-1}Mv_2 &  \ldots & P^{-1}Mv_n \end{spmatrix} \\
      &=  \begin{spmatrix}{c|c|c|c} \lambda P^{-1}v & P^{-1}Mv_2 &  \ldots & P^{-1}Mv_n \end{spmatrix} 
       =  \begin{spmatrix}{c|c|c|c} \lambda e_1 & P^{-1}Mv_2 &  \ldots & P^{-1}Mv_n \end{spmatrix} \\
      &=  \begin{spmatrix}{c|c|c|c} \lambda e_1 & w_2 &  \ldots & w_n \end{spmatrix}.
  \end{align*}
  Así, $N$ tiene la forma pedida y además $M \sim N$.

  Ahora si $M \sim N$ donde $N$ tiene la forma pedida, notemos que $Ne_1 = \lambda e_1$, eso muestra que $\lambda \in E(N)$, pero por el teorema \ref{teor:SemEspectro} tenemos que $E(M) = E(N)$ y por tanto $\lambda \in E(M)$.
\end{proof}

\begin{example}
  Sea $M$ la matriz con entradas reales de $2\times 2$ dada por 
  \[ \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}. \]
  Mostremos que no es triangularizable ni diagonalizable.

  \examplesolution

  En primer lugar, notemos que por el lema \ref{lema:CleanColumn1}, la matriz será triangularizable si y solo si la matriz tiene un valor propio. Así veamos cuales son sus valores propios usando el polinomio característico.
    \[
      \det(M-\lambda I) = \det \begin{pmatrix} - \lambda & -1 \\ 1 & - \lambda \end{pmatrix}
        = \lambda^2  + 1
    \]
  Por propiedades conocidas, es posible observar que no existen raíces reales para el polinomio característico. De este modo $M$ no tiene valores propios, por lo que no es triangularizable y por tanto tampoco es diagonalizable.
\end{example}


\begin{prop}\label{prop:MComplexHasEV}
  Sea $M \in  \M_n(\C)$, entonces $M$ tiene al menos un valor propio.
\end{prop}
\begin{proof}
  Sea $f \in \C[x]-\{0\}$ el polinomio minimal de $M$, por el teorema fundamental del cálculo, sabemos que debe haber alguna factorización $f = (x-\alpha_1)\cdots(x-\alpha_k)$, luego por la proposición \ref{prop:EvalPoly} tenemos que
  \[ f(M) = (M-\alpha_1 I)\cdots(M-\alpha_k I) = \bec 0.\]

  Ahora, dado que la multiplicación de matrices invertibles es invertible, entonces debe existir al menos un $\alpha_i$ con $i \in \{1,\ldots,k\}$ tal que $M-\alpha_i I$ no es invertible, ya que en caso contrario implicaría que $\bec 0$ es invertible. Pero por el teorema \ref{teor:PropVP}, esto implica que $\alpha_i$ es un valor propio de $M$.
\end{proof}

\begin{teor}\label{teor:ComplexTriang}
  Sea $M \in  \M_n(\C)$ entonces $M$ es triangularizable.
\end{teor}
\begin{proof}
  Probemos la proposición por inducción sobre $n$. Es claro que si $n = 1$ entonces $M$ ya es triangular superior, así supongamos que se cumple para algún natural $k$.

  Sea $M \in \M_{k+1}(\C)$, por la proposición \ref{prop:MComplexHasEV} existe $\lambda \in E(M)$, así, por el lema \ref{lema:CleanColumn1} entonces debe existir una matriz $Q \in \M_k(\C)$ y vector $v \in \F^k$ tal que $M \sim N$ donde 
  \[ N = \begin{spmatrix}{c|c}
      \lambda & v^t \\\hline
      \bec 0 & A
  \end{spmatrix} \]

  Pero por hipótesis de inducción existen matrices $U, Q \in \M_k(\C)$ donde $U$ es triangular superior y $Q$ es invertible tales que $A = Q^{-1}UQ$. Ahora, definamos las siguiente matríz $P$
  \[
    P = \begin{spmatrix}{c|c}
      1 & \bec 0 \\\hline
      \bec 0 & Q
  \end{spmatrix},
  \]
  Ahora, notemos que $P$ es una matriz de $(k+1)\times(k+1)$ invertible, ya que por multiplicación por bloques, se tiene que
  \[
    P \begin{spmatrix}{c|c}
      1 & \bec 0 \\\hline
      \bec 0 & Q^{-1}
  \end{spmatrix} = \begin{spmatrix}{c|c}
      1 & \bec 0 \\\hline
      \bec 0 & Q
  \end{spmatrix} \begin{spmatrix}{c|c}
    1 & \bec 0 \\\hline
    \bec 0 & Q^{-1}
\end{spmatrix} = \begin{spmatrix}{c|c}
  1 & \bec 0 \\\hline
  \bec 0 & I_k
\end{spmatrix} = I_{k+1}
      \implies
    P^{-1} = \begin{spmatrix}{c|c}
      1 & \bec 0 \\\hline
      \bec 0 & Q^{1}
  \end{spmatrix}.
  \]

  De esta forma, notemos que si definimos $T = P^{-1}NP$, entonces $M \sim T$, por transitividad de la semejanza y además se cumple que
  \begin{align*}
    T &= P^{-1}NP
       = \begin{spmatrix}{c|c}1 & \bec 0 \\\hline  \bec 0 & Q^{-1} \end{spmatrix}
         \begin{spmatrix}{c|c} \lambda & v^t \\\hline  \bec 0 & A \end{spmatrix}
         \begin{spmatrix}{c|c}1 & \bec 0 \\\hline  \bec 0 & Q \end{spmatrix} \\
      &= \begin{spmatrix}{c|c}1 & \bec 0 \\\hline  \bec 0 & Q^{-1} \end{spmatrix}
         \begin{spmatrix}{c|c}\lambda & v^tQ \\\hline  \bec 0 & AQ \end{spmatrix} 
       = \begin{spmatrix}{c|c}\lambda & v^tQ \\\hline  \bec 0 & Q^{-1}AQ \end{spmatrix} \\
      &= \begin{spmatrix}{c|c}\lambda & v^tQ \\\hline  \bec 0 & U \end{spmatrix}.
  \end{align*}
  Notemos que por la forma, $T$ es una matriz triangular superior. De esta forma tenemos que $M$ es triangularizable.

  De esta forma, por inducción, tenemos que para cualquier matriz cuadrada con entradas en los complejos es triangularizable.
\end{proof}



\subsection{Polinomio minimal de matrices complejas.}

Para concluir con este capítulo, mostraremos la forma del polinomio minimal para matrices compleja. Para ello necesitaremos recordar una propiedad importante de las matrices.

\begin{defi}
  Sea $M \in \M_n(\F)$, decimos que $M$ es \emph{nilpotente} si existe algún $d\in\N$ tal que $M^d = \bec 0$. Si $M$ es nilpotente, al mínimo $d \in \N$ tal que $M^d = \bec 0$ se le denomina \emph{grado del nilpotencia} de $M$.
\end{defi}

En primer lugar, notemos que si tenemos una matriz $T$ triangular no estricta, entonces $T$ no será nilpotente. Esto se puede comprobar fácilmente, para ello hay que recordar que si tenemos dos matrices triangulares superiores (inferiores) $A$ y $B$, entonces
  \[
    (AB)_{ii} = A_{ii} B_{ii}
  \]
para todo $i \in \{1,\ldots,n\}$. De este modo, si $T$ no es estricta, entonces para algún $j \in \{1,\ldots,n\}$ se debe cumplir que $T_{jj} \neq 0$ y por tanto $(T^k)_{ii} = (T_{ii})^k \neq 0$ para todo $k \in \N$.

De este modo, para que una matriz triangular sea nilpotente, tenemos que debe ser triangular estricta. La vuelta también es cierta y además podemos obtener una cota superior para su grado de nilpotencia.

\begin{lema}\label{lema:TriangNilp}
  Sea $T \in M_n(\F)$ una matriz triangular, entonces $T$ es nilpotente si y solo sí $T$ es triangular estricta. Además, si $T$ es nilpotente, entonces su grado de nilpotencia es menor o igual a $n$.
\end{lema}
\begin{proof}
  Por lo ya visto, sabemos que si $T$ es nilpotente entonces debe ser triangular estricta. De esta forma, solo falta ver que todas las matrices $T$ triangulares estrictas cumplen que $T^n = \bec 0$.

  Pero antes de ello, primero veamos que para cualesquiera matrices $A,B \in M_n(\F)$ y vectores $v,w \in \F^n$, entonces
  \begin{equation}\label{eq:auxProdBloq}
    \begin{spmatrix}{c|c}
      0 & v^t \\\hline
      \bec 0 & A
    \end{spmatrix}
    \begin{spmatrix}{c|c}
      0 & w^t \\\hline
      \bec 0 & B
    \end{spmatrix}
      = 
    \begin{spmatrix}{c|c}
      0 & v^tB \\\hline
      \bec 0 & AB
    \end{spmatrix}.
  \end{equation}

  Ahora, primero consideremos el caso donde $T$ es una matriz triangular superior estricta. Por inducción sobre $n$, notemos que si $n = 1$, entonces $T = 0$, por lo que es claro que $T^n = 0$. Así, supongamos que la propiedad se cumple para algún $k \in \N$.

  Sea $T \in \M_{k+1}(\F)$ una matriz triangular superior estricta, notemos que entonces $T$ tiene la siguiente forma
  \[
    T = \begin{spmatrix}{c|c}
      0 & v^t \\\hline
      \bec 0 & U
    \end{spmatrix},
  \]
  para alguna matriz triangular superior estricta $U \in \M_k(\F)$ y vector $v \in \F^k$. Ahora, por la ecuación \ref{eq:auxProdBloq} tenemos que
  \begin{align*}
    T^2 &= \begin{spmatrix}{c|c} 0 & v^t \\\hline \bec 0 & U \end{spmatrix} 
           \begin{spmatrix}{c|c} 0 & v^t \\\hline \bec 0 & U \end{spmatrix}
        = \begin{spmatrix}{c|c} 0 & v^tU \\\hline \bec 0 & U^2 \end{spmatrix} \\
    T^3 &= \begin{spmatrix}{c|c} 0 & v^tU \\\hline \bec 0 & U^2 \end{spmatrix} 
        \begin{spmatrix}{c|c} 0 & v^t \\\hline \bec 0 & U \end{spmatrix}
     = \begin{spmatrix}{c|c} 0 & v^tU^2 \\\hline \bec 0 & U^3 \end{spmatrix} \\
    &\vdotswithin{=} \\
    T^{k+1} &= \begin{spmatrix}{c|c} 0 & v^tU^{k-1} \\\hline \bec 0 & U^k \end{spmatrix} 
     \begin{spmatrix}{c|c} 0 & v^t \\\hline \bec 0 & U \end{spmatrix}
  = \begin{spmatrix}{c|c} 0 & v^tU^k \\\hline \bec 0 & U^{k+1} \end{spmatrix}.
  \end{align*}
  Ahora, por hipótesis de inducción tenemos que $U^k = U^{k+1} = \bec 0$ y de este modo $T^{k+1} = \bec 0$.

  Así, cualquier matriz $T \in M_n(\F)$  triangular superior estricta es nilpotente, y su grado de nilpotencia es menor o iguala a $n$.

  Ahora, si $T \in M_n(\F)$ es triangular inferior estricta, entonces $T^t$ es triangular superior estricta y por tanto $(T^t)^n = (T^n)^t = \bec 0$, lo que implica que $T^n =  \bec 0$. Lo que finalmente nos permite concluir la proposición.
\end{proof}

Para continuar, supongamos que tenemos una matriz triangular $T\in\M_n(\F)$. Por propiedades de la determinante sabemos que si $T = (t_{ij})$, entonces $\det(T) = t_{11} \cdots t_{nn}$.

De esta forma, si consideramos la matriz $T-t_{ii}I$ para alguna $i \in \{1,\ldots,n\}$, entonces, por propiedades de las matrices triangulares, tenemos que $T-t_{ii}I$ es triangular y además
\[
  \det(T-t_{ii}I) = (t_{11}-t_{ii})\cdots(t_{ii}-t_{ii})\cdots(t_{nn}-t_{ii}) = 0.
\]
Y por el teorema \ref{teor:PropVP}, eso quiere decir que $t_{ii} \in E(T)$. Ahora, de manera análoga, si $\lambda \in E(T)$ entonces 
  \[
    \det(T-t_{ii}I) = (t_{11}-\lambda)\cdots(t_{nn}-\lambda) = 0.
  \]
pero por propiedades de los campos, eso quiere decir que $\lambda = t_{ii}$ para algún $i \in \{1,\ldots,n\}$. Con esto, hemos mostrado la siguiente proposición

\begin{prop}\label{prop:EpecTriang}
  Sea $T \in \M_n(\F)$ una matriz triangular, entonces $E(T) = \{T_{ii} : i = 1,\ldots,n\}$. \qed
\end{prop}

Con todas estas propiedades ya demostradas, podemos demostrar el siguiente teorema.

\begin{teor}\label{teor:MinPolyTriang}
  Sea $T\in\M_n(\F)$ una matriz triangular, si $E(T) = \{\lambda_1,\ldots,\lambda_k\}$, entonces
    \[
      f_T = \prod_{i=1}^k (x-\lambda_i)^{n_i},
    \]
  donde $1 \leq n_i \leq n$ para cada $i \in \{1,\ldots,k\}$.
\end{teor}
\begin{proof}
  Sea $U = (T-\lambda_1 I)\cdots(T-\lambda_n I)$ y $T = (t_{ij})$. Primero, notemos que, por la proposición \ref{prop:EpecTriang}, para cada $i \in \{1,\ldots,n\}$ existe $j\in\{1,\ldots,k\}$ tal que $t_{ii} = \lambda_j$. De esta forma por propiedades de las matrices triangulares, se cumple que $U$ es triangular y además
  \begin{align*}
    U_{ii} &= (T-\lambda_1 I)_{ii}\cdots(T-\lambda_j I)_{ii}\cdots(T-\lambda_n I)_{ii}  \\
      &= (t_{ii}-\lambda_1)\cdots(t_{ii}-\lambda_j)\cdots(t_{ii}-\lambda_n)  \\
      &= 0.
  \end{align*}
  Así, tenemos que $U$ es una matriz triangular estricta y por el lema \ref{lema:TriangNilp} tenemos que $U^n = \bec 0$.

  Ahora, sea $f = (x-\lambda_1)^n\ldots(x-\lambda_k)^n$, notemos que $f(T) = U^n = \bec 0$, así, por el teorema \ref{teor:UnicidadPMin}, tenemos que $f_T \mid f$.
  
  Como $f$ se puede factorizar como un producto de polinomios de grado $1$, entonces, dado que $f_T$ divide a $f$, $f_T$ tiene coeficiente líder $1$ y por teoremas de divisibilidad de polinomios, se debe cumplir que
    \[
      f_T = \prod_{i=1}^k (x-\lambda_i)^{n_i},
    \]
donde $0 \leq n_i \leq n$ para cada $i \in \{1,\ldots,k\}$. Así supongamos que $n_r = 0$ para algún $r \in \{1,\ldots,k\}$.

Nuevamente por la proposición \ref{prop:EpecTriang} debe existir $s \in \{1,\ldots,n\}$ tal que $\lambda_r = t_{ss}$. Ahora, por propiedades de las matrices triangulares, se debe cumplir que
  \[
    [f_T(T)]_{ss} 
      = \prod_{\substack{i=1\\i\neq r}}^k (T-\lambda_i I)_{ss}^{n_i} 
      = \prod_{\substack{i=1\\i\neq r}}^k (t_{ss}-\lambda_i)^{n_i}.
  \]
Notemos que $t_{ss} \neq \lambda_i$ para todo $i \in \{1,\ldots,k\}-\{r\}$, entonces $[f_T(T)]_{ss} \neq 0$, pero eso contradice que $f_T(T) = \bec 0$. De este modo $1 \leq n_i \leq n$ para cada $i \in \{1,\ldots,k\}$.
\end{proof}

Ahora, como toda matriz compleja es triangularizable y el polinomio minimal es invariante ante la semejanza, el teorema anterior nos permite describir su polinomio minimal.

\begin{coro}
  Sea $M = \M_n(\C)$ y $E(M) = \{\lambda_1,\ldots,\lambda_k\}$, entonces $ f_M = \prod_{i=1}^k (x-\lambda_i)^{n_i}$, donde $1 \leq n_i \leq n$ para cada $i \in \{1,\ldots,k\}$.
\end{coro}
\begin{proof}
  Por el teorema \ref{teor:ComplexTriang}, sabemos que debe existir una matriz triangular $T \in \M_n(\C)$ tal que $M \sim T$. Ahora, por el teorema \ref{teor:SemEspectro} sabemos que $E(M) = E(T)$, así, por el teorema \ref{teor:MinPolyTriang} y la proposición \ref{prop:InvMinPoly}, finalmente obtenemos que
  \[
    f_M = f_T = \prod_{i=1}^k (x-\lambda_i)^{n_i},
  \]
donde $1 \leq n_i \leq n$ para cada $i \in \{1,\ldots,k\}$.
\end{proof}